{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "192bcd7f-309a-465b-8e56-fb9638149fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler , MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, LSTM, concatenate\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e6f6bd-0dc4-49c1-8677-b43bb287bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_excel('Classification_balanced_Categories.xlsx')# Data used in Classification \n",
    "data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e3d31b-629b-4a1d-92b3-058eccd017f6",
   "metadata": {},
   "source": [
    "# Data splitting, preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124363d0-1cbd-429c-90fe-f6247a2decab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get df values\n",
    "X = data.values\n",
    "X[X==0] = 1e-5\n",
    "\n",
    "\n",
    "\n",
    "X_train_base, X_test_base = train_test_split(X, test_size=0.2, random_state=100, shuffle=True)\n",
    "X_train_base, X_val_base = train_test_split(X_train_base, test_size=0.05, random_state=50, shuffle=False)\n",
    "\n",
    "# Normalization\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create and normalize train datasets for 1D\n",
    "list = [5] # Labels index\n",
    "num_time_steps = 100\n",
    "norm_log_t_train = scaler.fit_transform(np.log(X_train_base[:, 5:105].astype(float)))\n",
    "norm_log_p_train = scaler.fit_transform(np.log(X_train_base[:, 105:205].astype(float)))\n",
    "norm_log_dp_train = scaler.fit_transform(np.log(X_train_base[:, 205:305].astype(float)))\n",
    "norm_target_train = (X_train_base[:, list].astype(int))\n",
    "\n",
    "# Discard time, normalize logP, logdP for 2D\n",
    "X_train = np.stack([norm_log_p_train,   norm_log_dp_train], axis=2)  # Stacking 'dp' and 'derp'\n",
    "X_train_reshaped = X_train.reshape(-1, num_time_steps, 2)\n",
    "train_label = norm_target_train -1\n",
    "\n",
    "X_train_1 = np.hstack([ norm_log_t_train, norm_log_dp_train])#norm_log_p_train, norm_log_t_train,\n",
    "train_label = norm_target_train -1\n",
    "\n",
    "# Create and normalize test datasets 1D\n",
    "norm_t_test = scaler.fit_transform(np.log(X_test_base[:, 5:105].astype(float)))\n",
    "norm_log_p_test = scaler.fit_transform(np.log(X_test_base[:, 105:205].astype(float)))\n",
    "norm_log_dp_test = scaler.fit_transform(np.log(X_test_base[:, 205:305].astype(float)))\n",
    "norm_target_test = (X_test_base[:, list].astype(int))\n",
    "\n",
    "# Discard time, normalize logP, logdP 2D\n",
    "X_test = np.stack([norm_log_p_test,  norm_log_dp_test], axis=2)  # Stacking 'dp' and 'derp'\n",
    "X_test_reshaped = X_test.reshape(-1, num_time_steps, 2)\n",
    "test_label = norm_target_test \n",
    "\n",
    "X_test_1 = np.hstack([ norm_t_test, norm_log_dp_test])#norm_log_p_test, norm_t_test,\n",
    "test_label = norm_target_test -1\n",
    "\n",
    "# Create and normalize Val datasets\n",
    "norm_t_val = scaler.fit_transform(np.log(X_val_base[:, 5:105].astype(float)))\n",
    "norm_log_p_val = scaler.fit_transform(np.log(X_val_base[:, 105:205].astype(float)))\n",
    "norm_log_dp_val = scaler.fit_transform(np.log(X_val_base[:, 205:305].astype(float)))\n",
    "norm_target_val = (X_val_base[:, list].astype(int))\n",
    "\n",
    "# Discard time, normalize logP, logdP\n",
    "X_val = np.stack([norm_log_p_val,  norm_log_dp_val], axis=2)  # Stacking 'dp' and 'derp'\n",
    "X_val_reshaped = X_val.reshape(-1, num_time_steps, 2)\n",
    "val_label = norm_target_val\n",
    "\n",
    "X_val_1 = np.hstack([norm_t_val,  norm_log_dp_val])#norm_log_p_val,\n",
    "val_label = norm_target_val -1\n",
    "\n",
    "\n",
    "# 1D\n",
    "train_sets1 = [X_train_1, train_label] # can add the lebel to the array \",train_label\"\n",
    "test_sets1  = [X_test_1, test_label]\n",
    "val_sets1  = [X_val_1, val_label]\n",
    "\n",
    "# 2D\n",
    "train_sets2 = [X_train, test_label] # can add the lebel to the array \",train_label\"\n",
    "test_sets2  = [X_test, train_label]\n",
    "val_sets2  = [X_val, val_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b9447c-37ff-4f4b-a736-60a8a40c103b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d7b4658-92cc-47d7-b2b6-3d4ae5d89def",
   "metadata": {},
   "source": [
    "# 1 num_conv_layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5958e822-ec42-4147-83d6-03c5f1918ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dense, BatchNormalization, Flatten, Activation\n",
    "\n",
    "def create_model(num_conv_layers):\n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(Conv1D(filters=128, kernel_size=7, activation='relu', input_shape=(200, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Additional Conv layers based on num_conv_layers parameter\n",
    "    for _ in range(1, num_conv_layers):\n",
    "        model.add(Conv1D(filters=128, kernel_size=3 if _ % 2 == 0 else 5, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Flatten and Dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Creating models with different numbers of convolutional layers\n",
    "models = {}\n",
    "for i in range(1, 4):  # Assuming the original model had 3 Conv1D layers\n",
    "    models[f'model_with_{i}_conv_layers'] = create_model(i)\n",
    "\n",
    "# Now, you can train and evaluate each model separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1ed498-88e9-4658-ba51-bc56ccfe495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = {}\n",
    "for key, model in models.items():\n",
    "    print(f\"Training {key}...\")\n",
    "    history = model.fit(train_sets1[0], train_sets1[1], epochs=50, validation_data=(test_sets1[0], test_sets1[1]), batch_size=20)\n",
    "    history_dict[key] = history\n",
    "    model.evaluate(test_sets1[0], test_sets1[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7c8977-351f-4d6a-b0c8-e60fb7404140",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plot settings\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m      3\u001b[0m colors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Different colors for each model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 Conv Layer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2 Conv Layers\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3 Conv Layers\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Labels for each line\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot settings\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['blue', 'green', 'red']  # Different colors for each model\n",
    "labels = ['1 Conv Layer', '2 Conv Layers', '3 Conv Layers']  # Labels for each line\n",
    "\n",
    "# Loop through the history of each model and plot\n",
    "for (key, history), color, label in zip(history_dict.items(), colors, labels):\n",
    "    epochs = range(1, 51)  # 10 epochs, adjust if necessary\n",
    "    training_loss = history.history['loss']\n",
    "    validation_loss = history.history['val_loss']\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    #plt.plot(epochs, training_loss, 'o-', color=color, label=f'Training Loss - {label}')\n",
    "    plt.plot(epochs, validation_loss, 's--', color=color, label=f'Validation Loss - {label}')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Validation Loss by Convolutional Layer Count Over Epochs')\n",
    "plt.xlabel('Epochs',fontsize=14)\n",
    "plt.ylabel('Loss',fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "# Adding grid with major and minor lines\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='gray')  # Major grid\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')  # Minor grid\n",
    "plt.minorticks_on()  # Enable minor ticks\n",
    "\n",
    "# Save the figure at a high resolution\n",
    "plt.savefig('Conv_layer_variation_effect_classification.png', dpi=1200)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e75f5c9-4ad6-41ae-871f-6ccf5736cded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f392361-282b-4203-b379-4716f2c21677",
   "metadata": {},
   "source": [
    "# 2 Kernel_variation_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3cfe0-dd62-4218-a89c-002815630016",
   "metadata": {},
   "outputs": [],
   "source": [
    "From keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dense, BatchNormalization, Flatten\n",
    "\n",
    "def create_model(kernel_sizes):\n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(Conv1D(filters=128, kernel_size=kernel_sizes[0], activation='relu', input_shape=(200, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Additional Conv layers based on kernel_sizes list\n",
    "    for size in kernel_sizes[1:]:\n",
    "        model.add(Conv1D(filters=128, kernel_size=size, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Flatten and Dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define different sets of kernel sizes for the experiment\n",
    "kernel_configurations = [\n",
    "    [7, 5, 3],  # Original sizes\n",
    "    [3, 3, 3],  # Smaller kernels\n",
    "    [9, 7, 5]   # Larger kernels\n",
    "]\n",
    "\n",
    "# Creating models with different kernel sizes\n",
    "models = {}\n",
    "for i, sizes in enumerate(kernel_configurations, start=1):\n",
    "    model_key = f'model_with_kernel_sizes_{sizes}'\n",
    "    models[model_key] = create_model(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d000d66f-2ea7-4971-8b2a-75cbb81bf5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each model\n",
    "history_dict = {}\n",
    "for key, model in models.items():\n",
    "    print(f\"Training {key}...\")\n",
    "    history = model.fit(train_sets1[0], train_sets1[1], epochs=50, validation_data=(test_sets1[0], test_sets1[1]), batch_size=20)\n",
    "    history_dict[key] = history\n",
    "    evaluation = model.evaluate(test_sets1[0], test_sets1[1])\n",
    "    print(f\"Evaluation {key}: Loss = {evaluation[0]}, Accuracy = {evaluation[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b7f48-bac6-4262-967d-15cf818fefdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['blue', 'green', 'red']  # Different colors for each model configuration\n",
    "labels = ['Kernel Sizes: [7, 5, 3]', 'Kernel Sizes: [3, 3, 3]', 'Kernel Sizes: [9, 7, 5]']  # Labels based on kernel sizes\n",
    "\n",
    "# Assuming history_dict is correctly updated to include models keyed by their kernel configuration\n",
    "# Loop through the history of each model and plot\n",
    "for (key, history), color, label in zip(history_dict.items(), colors, labels):\n",
    "    epochs = range(1, 51)  # Adjust the range based on the actual number of epochs trained\n",
    "    validation_loss = history.history['val_loss']\n",
    "\n",
    "    # Plot validation loss\n",
    "    plt.plot(epochs, validation_loss, 's--', color=color, label=f'Validation Loss - {label}')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Validation Loss for Different Kernel Configurations Over Epochs', fontsize=16)\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "# Adding grid with major and minor lines\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='gray')  # Major grid\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')  # Minor grid\n",
    "plt.minorticks_on()  # Enable minor ticks\n",
    "\n",
    "# Save the figure at a high resolution\n",
    "plt.savefig('Kernel_variation_effect_classification.png', dpi=1200)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7565d281-ce2f-4240-aefb-11e4791635c6",
   "metadata": {},
   "source": [
    "# 3 Pooling configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf125d-329a-4a59-952c-a1c54732154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, AveragePooling1D, Dense, BatchNormalization, Flatten\n",
    "\n",
    "def create_model(kernel_sizes, pool_size, pooling_type='average'):\n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(Conv1D(filters=128, kernel_size=kernel_sizes[0], activation='relu', input_shape=(200, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    if pooling_type == 'average':\n",
    "        model.add(AveragePooling1D(pool_size=pool_size))\n",
    "    else:\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    \n",
    "    # Additional Conv layers based on kernel_sizes list\n",
    "    for size in kernel_sizes[1:]:\n",
    "        model.add(Conv1D(filters=128, kernel_size=size, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        if pooling_type == 'average':\n",
    "            model.add(AveragePooling1D(pool_size=pool_size))\n",
    "        else:\n",
    "            model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    \n",
    "    # Flatten and Dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Pooling configurations for the experiment\n",
    "pooling_configurations = [\n",
    "    {'pool_size': 2, 'pooling_type': 'average'},\n",
    "    {'pool_size': 3, 'pooling_type': 'average'},\n",
    "    {'pool_size': 2, 'pooling_type': 'max'}\n",
    "]\n",
    "\n",
    "# Kernel sizes used for each model\n",
    "kernel_sizes = [7, 5, 3]  # Adjust as necessary based on previous configurations\n",
    "\n",
    "# Creating models with different pooling strategies and sizes\n",
    "models = {}\n",
    "for config in pooling_configurations:\n",
    "    model_key = f\"model_pool_{config['pool_size']}_{config['pooling_type']}\"\n",
    "    models[model_key] = create_model(kernel_sizes, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac5f4a6-c5e6-429c-968e-6ccc51b1aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each model\n",
    "history_dict = {}\n",
    "for key, model in models.items():\n",
    "    print(f\"Training {key}...\")\n",
    "    history = model.fit(train_sets1[0], train_sets1[1], epochs=100, validation_data=(test_sets1[0], test_sets1[1]), batch_size=20)\n",
    "    history_dict[key] = history\n",
    "    evaluation = model.evaluate(test_sets1[0], test_sets1[1])\n",
    "    print(f\"Evaluation {key}: Loss = {evaluation[0]}, Accuracy = {evaluation[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7734b5ab-11f8-482d-a4e5-4c7830cdd7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['blue', 'green', 'red']  # Different colors for each pooling configuration\n",
    "labels = [\n",
    "    'Average Pooling Size 2', \n",
    "    'Average Pooling Size 3', \n",
    "    'Max Pooling Size 2'\n",
    "]  # Labels based on pooling type and size\n",
    "\n",
    "# Loop through the history of each model and plot\n",
    "for (key, history), color, label in zip(history_dict.items(), colors, labels):\n",
    "    epochs = range(1, 101)  # Adjust the range based on the actual number of epochs trained\n",
    "    validation_loss = history.history['val_loss']\n",
    "\n",
    "    # Plot validation loss\n",
    "    plt.plot(epochs, validation_loss, 'o--', color=color, label=f'{label} Validation Loss')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Impact of Pooling Strategies on Validation Loss Over Epochs', fontsize=16)\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "# Adding grid with major and minor lines\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='gray')  # Major grid\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')  # Minor grid\n",
    "plt.minorticks_on()  # Enable minor ticks\n",
    "\n",
    "# Save the figure at a high resolution\n",
    "plt.savefig('Impact_of_Pooling_Strategies_classification.png', dpi=1200)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7cd384-c868-4583-97f7-b3209de834e8",
   "metadata": {},
   "source": [
    "# 4 Impact of Neuron Counts in Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848dfb84-1c80-45bd-bc35-c9389c64a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dense, BatchNormalization, Flatten\n",
    "\n",
    "def create_model(kernel_sizes, dense_neurons):\n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(Conv1D(filters=128, kernel_size=kernel_sizes[0], activation='relu', input_shape=(200, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Additional Conv layers based on kernel_sizes list\n",
    "    for size in kernel_sizes[1:]:\n",
    "        model.add(Conv1D(filters=128, kernel_size=size, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Flatten the output before passing it to the Dense layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Adjustable Dense layer\n",
    "    model.add(Dense(dense_neurons, activation='relu'))  # Adjusting the number of neurons\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Different neuron counts in the dense layer\n",
    "neuron_configurations = [64, 128, 256]  # Example configurations\n",
    "\n",
    "# Creating models with different numbers of neurons in the dense layer\n",
    "models = {}\n",
    "for neurons in neuron_configurations:\n",
    "    model_key = f\"model_with_{neurons}_neurons\"\n",
    "    models[model_key] = create_model([7, 5, 3], neurons)  # Assuming kernel sizes are fixed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23630e22-5bf9-4b33-8b96-b6c2eebd87b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each model\n",
    "history_dict = {}\n",
    "for key, model in models.items():\n",
    "    print(f\"Training {key}...\")\n",
    "    history = model.fit(train_sets1[0], train_sets1[1], epochs=100, validation_data=(test_sets1[0], test_sets1[1]), batch_size=20)\n",
    "    history_dict[key] = history\n",
    "    evaluation = model.evaluate(test_sets1[0], test_sets1[1])\n",
    "    print(f\"Evaluation {key}: Loss = {evaluation[0]}, Accuracy = {evaluation[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e299b-8009-4999-af1c-50b2efb2cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['blue', 'green', 'red']  # Different colors for each neuron configuration\n",
    "labels = ['64 Neurons', '128 Neurons', '256 Neurons']  # Labels based on neuron counts in the dense layer\n",
    "\n",
    "# Loop through the history of each model and plot\n",
    "for (key, history), color, label in zip(history_dict.items(), colors, labels):\n",
    "    epochs = range(1, 101)  # Adjust the range based on the actual number of epochs trained\n",
    "    validation_loss = history.history['val_loss']\n",
    "\n",
    "    # Plot validation loss\n",
    "    plt.plot(epochs, validation_loss, 'o-', color=color, label=f'{label} Validation Loss')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Impact of Neuron Counts in Dense Layer on Validation Loss Over Epochs', fontsize=16)\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "# Adding grid with major and minor lines\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='gray')  # Major grid\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')  # Minor grid\n",
    "plt.minorticks_on()  # Enable minor ticks\n",
    "\n",
    "# Save the figure at a high resolution\n",
    "plt.savefig('Impact_of_Neuron_Counts_classification.png', dpi=1200)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb65c165-f7c2-445e-b12f-12ec0dfe8fed",
   "metadata": {},
   "source": [
    "# 5 Impact of Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3888a4-6779-4a14-b204-f2f4ca6b9489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dense, BatchNormalization, Flatten\n",
    "\n",
    "def create_model(kernel_sizes, activation_function):\n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(Conv1D(filters=128, kernel_size=kernel_sizes[0], activation=activation_function, input_shape=(200, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Additional Conv layers based on kernel_sizes list\n",
    "    for size in kernel_sizes[1:]:\n",
    "        model.add(Conv1D(filters=128, kernel_size=size, activation=activation_function))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Flatten the output before passing it to the Dense layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Dense layer with configurable activation function\n",
    "    model.add(Dense(128, activation=activation_function))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Different activation functions to test\n",
    "activation_functions = ['relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Creating models with different activation functions\n",
    "models = {}\n",
    "for activation in activation_functions:\n",
    "    model_key = f\"model_with_{activation}_activation\"\n",
    "    models[model_key] = create_model([7, 5, 3], activation)  # Assuming kernel sizes are fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a4da12-99b5-4450-8a8e-fdad2991a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each model\n",
    "history_dict = {}\n",
    "for key, model in models.items():\n",
    "    print(f\"Training {key}...\")\n",
    "    history = model.fit(train_sets1[0], train_sets1[1], epochs=100, validation_data=(test_sets1[0], test_sets1[1]), batch_size=20)\n",
    "    history_dict[key] = history\n",
    "    evaluation = model.evaluate(test_sets1[0], test_sets1[1])\n",
    "    print(f\"Evaluation {key}: Loss = {evaluation[0]}, Accuracy = {evaluation[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bcff72-3389-4dda-869e-7baf5aa80a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['blue', 'green', 'red']  # Different colors for each activation function\n",
    "labels = ['ReLU Activation', 'Sigmoid Activation', 'Tanh Activation']  # Labels based on activation functions\n",
    "\n",
    "# Loop through the history of each model and plot\n",
    "for (key, history), color, label in zip(history_dict.items(), colors, labels):\n",
    "    epochs = range(1, 101)  # Adjust the range based on the actual number of epochs trained\n",
    "    validation_loss = history.history['val_loss']\n",
    "\n",
    "    # Plot validation loss\n",
    "    plt.plot(epochs, validation_loss, 'o-', color=color, label=f'{label} Validation Loss')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Impact of Activation Functions on Validation Loss Over Epochs', fontsize=16)\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "# Adding grid with major and minor lines\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='gray')  # Major grid\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')  # Minor grid\n",
    "plt.minorticks_on()  # Enable minor ticks\n",
    "\n",
    "# Save the figure at a high resolution\n",
    "plt.savefig('Impact_of_ Activation_Functions_classification.png', dpi=1200)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d82c4-2aed-4e3f-af6a-6713a3e39a89",
   "metadata": {},
   "source": [
    "# 6 Batch Normalization Influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337684a8-5bb1-4e00-bee6-f150f7d15764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(kernel_sizes, use_batch_norm=True):\n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(Conv1D(filters=128, kernel_size=kernel_sizes[0], activation='relu', input_shape=(200, 1)))\n",
    "    if use_batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Additional Conv layers based on kernel_sizes list\n",
    "    for size in kernel_sizes[1:]:\n",
    "        model.add(Conv1D(filters=128, kernel_size=size, activation='relu'))\n",
    "        if use_batch_norm:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Flatten the output before passing it to the Dense layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Dense layer\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Batch normalization configurations for the experiment\n",
    "batch_norm_configurations = [True, False]\n",
    "\n",
    "# Kernel sizes used for each model\n",
    "kernel_sizes = [3, 3, 3]  # Adjust as necessary based on previous configurations\n",
    "\n",
    "# Creating models with and without batch normalization\n",
    "models = {}\n",
    "for use_batch_norm in batch_norm_configurations:\n",
    "    model_key = f\"model_with_batch_norm_{use_batch_norm}\"\n",
    "    models[model_key] = create_model(kernel_sizes, use_batch_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f69d498-2361-4302-bf72-edb9e31c801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each model\n",
    "history_dict = {}\n",
    "for key, model in models.items():\n",
    "    print(f\"Training {key}...\")\n",
    "    history = model.fit(train_sets1[0], train_sets1[1], epochs=100, validation_data=(test_sets1[0], test_sets1[1]), batch_size=20)\n",
    "    history_dict[key] = history\n",
    "    evaluation = model.evaluate(test_sets1[0], test_sets1[1])\n",
    "    print(f\"Evaluation {key}: Loss = {evaluation[0]}, Accuracy = {evaluation[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df089db-e203-4ec5-bd84-c29b984e6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['blue', 'green']  # Different colors for with and without Batch Normalization\n",
    "labels = ['With Batch Normalization', 'Without Batch Normalization']  # Labels based on Batch Normalization usage\n",
    "\n",
    "# Loop through the history of each model and plot\n",
    "for (key, history), color, label in zip(history_dict.items(), colors, labels):\n",
    "    epochs = range(1, 101)  # Adjust the range based on the actual number of epochs trained\n",
    "    validation_loss = history.history['val_loss']\n",
    "\n",
    "    # Plot validation loss\n",
    "    plt.plot(epochs, validation_loss, 'o-', color=color, label=f'{label} Validation Loss')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Impact of Batch Normalization on Validation Loss Over Epochs', fontsize=16)\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "# Adding grid with major and minor lines\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='gray')  # Major grid\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')  # Minor grid\n",
    "plt.minorticks_on()  # Enable minor ticks\n",
    "\n",
    "# Save the figure at a high resolution\n",
    "plt.savefig('Impact_of_ Batch_Normalization_classification.png', dpi=1200)\n",
    "\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6607a61-0d1e-4429-a0d4-b75c5b4784cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7c628-d704-4af8-a568-446d90b09345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2decbbc3-ec94-455a-ab02-301b1b22a936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c130a60-df14-491d-8165-a86a166af586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Sample data: time and pressure\n",
    "time_pressure = np.array([0, 1, 2, 3, 4, 5])  # Common time points for the pressure\n",
    "pressure = np.array([10, 11, 9, 13, 12, 14])  # Pressure readings\n",
    "\n",
    "# Sample data: time and derivative\n",
    "time_derivative = np.array([0.5, 1.5, 2.5, 3.5, 4.5])  # Different time points\n",
    "derivative = np.array([0.1, -0.2, 0.15, -0.05, 0.1])  # Derivative readings\n",
    "\n",
    "# Interpolation function for the derivative\n",
    "interpolation_function = interp1d(time_derivative, derivative, kind='linear', fill_value=\"extrapolate\")\n",
    "\n",
    "# Interpolated derivative values at the pressure time points\n",
    "interpolated_derivative = interpolation_function(time_pressure)\n",
    "\n",
    "print(\"Interpolated Derivative Values:\", interpolated_derivative)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
